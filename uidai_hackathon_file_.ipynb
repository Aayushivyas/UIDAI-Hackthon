{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f326b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preliminary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import re\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7a5f95",
   "metadata": {},
   "source": [
    "### **AADHAR ENROLMENT DATASET**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fc8ba7",
   "metadata": {},
   "source": [
    "This dataset provides aggregated information on **Aadhaar enrolments** across various demographic and geographic levels. It includes variables such as the date of enrollment, state, district, PIN code, and age-wise categories (0–5 years, 5–17 years, and 18 years and above). The dataset captures both temporal and spatial patterns of enrolment activity, enabling detailed descriptive, comparative, and trend analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecb0a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "enrol_df1 = pd.read_csv(r\"C:\\Users\\Abhay\\OneDrive\\Desktop\\Hackathons\\UIDAI\\api_data_aadhar_enrolment_0_500000.csv\")\n",
    "enrol_df2 = pd.read_csv(r\"C:\\Users\\Abhay\\OneDrive\\Desktop\\Hackathons\\UIDAI\\api_data_aadhar_enrolment_500000_1000000.csv\")\n",
    "enrol_df3 = pd.read_csv(r\"C:\\Users\\Abhay\\OneDrive\\Desktop\\Hackathons\\UIDAI\\api_data_aadhar_enrolment_1000000_1006029.csv\")\n",
    "\n",
    "#Merging datasets one below another\n",
    "enrol_df = pd.concat([enrol_df1, enrol_df2, enrol_df3], ignore_index=True)\n",
    "print(f\"\\nShape of enrol_df1:{enrol_df1.shape}\")\n",
    "print(f\"\\nShape of enrol_df2:{enrol_df2.shape}\")\n",
    "print(f\"\\nShape of enrol_df3:{enrol_df3.shape}\")\n",
    "print(f\"\\nShape of enrol_df:{enrol_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96137811",
   "metadata": {},
   "outputs": [],
   "source": [
    "enrol_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d8323d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = enrol_df[enrol_df['state'] == '100000']\n",
    "print(f\"{x['age_0_5'].sum()}\\n\")\n",
    "print(f\"{x['age_5_17'].sum()}\\n\")\n",
    "print(f\"{x['age_18_greater'].sum()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc7c8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "enrol_df_groupby_district = enrol_df.groupby(['state','district']).agg(Age_0_5=('age_0_5','sum'), Age_5_17=('age_5_17','sum'), Age_18_plus=('age_18_greater','sum')).reset_index()\n",
    "enrol_df_groupby_district.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad6ff19",
   "metadata": {},
   "outputs": [],
   "source": [
    "enrol_df_groupby_state = enrol_df.groupby('state').agg(Age_0_5=('age_0_5','sum'), Age_5_17=('age_5_17','sum'), Age_18_plus=('age_18_greater','sum')).reset_index()\n",
    "enrol_df_groupby_state['total'] = enrol_df_groupby_state['Age_0_5'] + enrol_df_groupby_state['Age_5_17'] + enrol_df_groupby_state['Age_18_plus']\n",
    "enrol_df_groupby_state.sort_values(by='total', ascending=False, inplace=True)\n",
    "enrol_df_groupby_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed814d0a",
   "metadata": {},
   "source": [
    "### **Aadhaar Demographic Update dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8daa2c69",
   "metadata": {},
   "source": [
    "This dataset captures aggregated information related to **updates made to residents’ demographic data linked to Aadhaar**, such as name, address, date of birth, gender, and mobile number. It provides insights into the frequency and distribution of demographic changes across different time periods and geographic levels (state, district, and PIN code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068a7f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "demographic_df1 = pd.read_csv(r\"C:\\Users\\Abhay\\OneDrive\\Desktop\\Hackathons\\UIDAI\\api_data_aadhar_demographic_0_500000.csv\")\n",
    "demographic_df2 = pd.read_csv(r\"C:\\Users\\Abhay\\OneDrive\\Desktop\\Hackathons\\UIDAI\\api_data_aadhar_demographic_500000_1000000.csv\")\n",
    "demographic_df3 = pd.read_csv(r\"C:\\Users\\Abhay\\OneDrive\\Desktop\\Hackathons\\UIDAI\\api_data_aadhar_demographic_1000000_1500000.csv\")\n",
    "demographic_df4 = pd.read_csv(r\"C:\\Users\\Abhay\\OneDrive\\Desktop\\Hackathons\\UIDAI\\api_data_aadhar_demographic_1500000_2000000.csv\")\n",
    "demographic_df5 = pd.read_csv(r\"C:\\Users\\Abhay\\OneDrive\\Desktop\\Hackathons\\UIDAI\\api_data_aadhar_demographic_2000000_2071700.csv\")\n",
    "\n",
    "#Merging datasets one below another\n",
    "demographic_df = pd.concat([demographic_df1, demographic_df2, demographic_df3, demographic_df4, demographic_df5], ignore_index=True)\n",
    "print(f\"\\nShape of demographic_df1:{demographic_df1.shape}\")\n",
    "print(f\"\\nShape of demographic_df2:{demographic_df2.shape}\")\n",
    "print(f\"\\nShape of demographic_df3:{demographic_df3.shape}\")\n",
    "print(f\"\\nShape of demographic_df4:{demographic_df4.shape}\")\n",
    "print(f\"\\nShape of demographic_df5:{demographic_df5.shape}\")\n",
    "print(f\"\\nShape of demographic_df:{demographic_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41906a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "demographic_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fa0b50",
   "metadata": {},
   "source": [
    "### **Aadhaar Biometric Update dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf3e884",
   "metadata": {},
   "source": [
    "This dataset contains aggregated information on biometric updates (modalities such as fingerprints, iris, and face). It reflects the periodic revalidation or correction of biometric details, especially for children transitioning into adulthood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ec0caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "biometric_df1 = pd.read_csv(r\"C:\\Users\\Abhay\\OneDrive\\Desktop\\Hackathons\\UIDAI\\api_data_aadhar_biometric_0_500000.csv\")\n",
    "biometric_df2 = pd.read_csv(r\"C:\\Users\\Abhay\\OneDrive\\Desktop\\Hackathons\\UIDAI\\api_data_aadhar_biometric_500000_1000000.csv\")\n",
    "biometric_df3 = pd.read_csv(r\"C:\\Users\\Abhay\\OneDrive\\Desktop\\Hackathons\\UIDAI\\api_data_aadhar_biometric_1000000_1500000.csv\")\n",
    "biometric_df4 = pd.read_csv(r\"C:\\Users\\Abhay\\OneDrive\\Desktop\\Hackathons\\UIDAI\\api_data_aadhar_biometric_1500000_1861108.csv\")\n",
    "\n",
    "#Merging datasets one below another\n",
    "biometric_df = pd.concat([biometric_df1, biometric_df2, biometric_df3, biometric_df4], ignore_index=True)\n",
    "print(f\"\\nShape of biometric_df1:{biometric_df1.shape}\")\n",
    "print(f\"\\nShape of biometric_df2:{biometric_df2.shape}\")\n",
    "print(f\"\\nShape of biometric_df3:{biometric_df3.shape}\")\n",
    "print(f\"\\nShape of biometric_df4:{biometric_df4.shape}\")\n",
    "print(f\"\\nShape of biometric_df:{biometric_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e61ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "biometric_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f09c81",
   "metadata": {},
   "source": [
    "### **Parsing Date**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8758e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_date(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"date\"])\n",
    "    df[\"month\"] = df[\"date\"].dt.to_period(\"M\").dt.to_timestamp()  # month start\n",
    "    return df\n",
    "\n",
    "bio  = _parse_date(biometric_df)\n",
    "demo = _parse_date(demographic_df)\n",
    "enr  = _parse_date(enrol_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfd76db",
   "metadata": {},
   "source": [
    "### **Age column to numeric**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b0fc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#age column to numeric\n",
    "def _age_cols(df: pd.DataFrame):\n",
    "    base = {\"date\", \"month\", \"state\", \"district\", \"pincode\"}\n",
    "    cols = [c for c in df.columns if c not in base]\n",
    "    # keep only numeric-like columns\n",
    "    num_cols = []\n",
    "    for c in cols:\n",
    "        if pd.api.types.is_numeric_dtype(df[c]) or df[c].astype(str).str.match(r\"^-?\\d+(\\.\\d+)?$\").mean() > 0.8:\n",
    "            num_cols.append(c)\n",
    "    # force numeric\n",
    "    df[num_cols] = df[num_cols].apply(pd.to_numeric, errors=\"coerce\").fillna(0)\n",
    "    return num_cols\n",
    "\n",
    "BIO_AGE  = _age_cols(bio)\n",
    "DEMO_AGE = _age_cols(demo)\n",
    "ENR_AGE  = _age_cols(enr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4f8aea",
   "metadata": {},
   "source": [
    "### **Monthly Aggregate at Pin/District**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3147d6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def monthly_aggregate(df: pd.DataFrame, level: str, value_cols: list[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    level: \"pin\" or \"district\"\n",
    "    returns monthly sums by level.\n",
    "    \"\"\"\n",
    "    if level == \"pin\":\n",
    "        keys = [\"month\", \"state\", \"district\", \"pincode\"] #group by month + state + district + pincode\n",
    "    elif level == \"district\":\n",
    "        keys = [\"month\", \"state\", \"district\"]            #group by month + state + district\n",
    "    else:\n",
    "        raise ValueError(\"level must be 'pin' or 'district'\")\n",
    "\n",
    "    out = (\n",
    "        df.groupby(keys, as_index=False)[value_cols]\n",
    "          .sum()\n",
    "          .sort_values(keys)\n",
    "          .reset_index(drop=True)\n",
    "    )\n",
    "    # total across age columns (handy for spikes/forecast)\n",
    "    out[\"total\"] = out[value_cols].sum(axis=1)\n",
    "    return out\n",
    "\n",
    "bio_pin_m   = monthly_aggregate(bio,  \"pin\",      BIO_AGE)\n",
    "bio_dist_m  = monthly_aggregate(bio,  \"district\", BIO_AGE)\n",
    "demo_pin_m  = monthly_aggregate(demo, \"pin\",      DEMO_AGE)\n",
    "demo_dist_m = monthly_aggregate(demo, \"district\", DEMO_AGE)\n",
    "enr_pin_m   = monthly_aggregate(enr,  \"pin\",      ENR_AGE)\n",
    "enr_dist_m  = monthly_aggregate(enr,  \"district\", ENR_AGE)\n",
    "\n",
    "# ----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57e1881",
   "metadata": {},
   "source": [
    "### **System wide Aadhar activity by Age Group**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f16776b",
   "metadata": {},
   "outputs": [],
   "source": [
    "enr_pin_m.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78fab5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_enr = enr_pin_m.copy()\n",
    "age_cols = ENR_AGE\n",
    "\n",
    "monthly = df_enr.groupby(\"month\", as_index=False)[age_cols].sum()\n",
    "\n",
    "plt.figure(figsize=(11, 5))\n",
    "\n",
    "plt.stackplot(\n",
    "    monthly[\"month\"],\n",
    "    monthly[\"age_0_5\"],\n",
    "    monthly[\"age_5_17\"],\n",
    "    labels=[\"Age 0–5\", \"Age 5–17\"],\n",
    "    colors=[\"#FFA07A\", \"#20B2AA\"]\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Total Activity\")\n",
    "plt.title(\"January spike indicates system-wide surge; post-Jan enrolment volumes stabilize\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(loc=\"upper right\")\n",
    "\n",
    "# --- annotation (add AFTER plotting, BEFORE tight_layout/show)\n",
    "total_by_month = monthly[[\"age_0_5\", \"age_5_17\"]].sum(axis=1)\n",
    "\n",
    "# If you specifically want to mark the first month (as in your snippet)\n",
    "plt.annotate(\n",
    "    \"System-wide spike\",\n",
    "    xy=(monthly[\"month\"].iloc[0], total_by_month.iloc[0]),\n",
    "    xytext=(monthly[\"month\"].iloc[1], total_by_month.max() * 0.9),\n",
    "    arrowprops=dict(arrowstyle=\"->\")\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c600d452",
   "metadata": {},
   "source": [
    "### **Spike detection/Severity Scoring**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90708ef5",
   "metadata": {},
   "source": [
    "The severity score is constructed as a multiplicative composite of three interpretable components: \n",
    "\n",
    "* relative magnitude (absolute month-over-month percent change), \n",
    "* statistical unusualness (a robust z-score of the month-to-month change using median and MAD), and \n",
    "* impact/scale (a log-transformed current-month volume). \n",
    "\n",
    "This design reflects a standard “risk = likelihood × impact” philosophy: the robust z-score captures how unlikely the change is relative to the location’s own historical behavior (reducing sensitivity to outliers and non-normality), the percent change captures how large the shift is in relative terms, and the log-volume term ensures the score reflects real-world operational impact without letting very large regions dominate purely by size.\n",
    "\n",
    "Severity score can be as a prioritization and response tool. The score helps identify which events demand immediate attention because they combine large change, rarity, and high operational impact. High-severity spikes can trigger actions such as surge staffing, temporary capacity expansion, targeted audits, or advance planning for expected drives. Aggregated over time and locations, severity scores also reveal districts that experience intense stress during spike months, informing where contingency plans, buffers, and preventive interventions will deliver the greatest benefit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1fef68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_spike_metrics(monthly_df: pd.DataFrame, group_keys: list[str], value_col: str = \"total\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Adds MoM absolute/percent changes + robust z-score on change + severity score.\n",
    "    severity = (abs(pct_change) capped) * robust_z_change * log1p(current_value)\n",
    "    \"\"\"\n",
    "    df = monthly_df.copy()\n",
    "    df = df.sort_values(group_keys + [\"month\"])\n",
    "\n",
    "    # MoM changes per group\n",
    "    #Create a groupby object (one group = one PIN time series\n",
    "    #g represents multiple separate time series:\n",
    "    #one for each unique (state, district, pincode) combination.\n",
    "    g = df.groupby(group_keys, sort=False)\n",
    "\n",
    "    #For each group (PIN let's say)\n",
    "    #-moves the values down by 1 row\n",
    "    #-so current row can reference its previous month’s value\n",
    "    df[\"prev\"] = g[value_col].shift(1)                #last month’s total\n",
    "    df[\"abs_change\"] = df[value_col] - df[\"prev\"]     #current_total − prev_total\n",
    "    #If a PIN has its first month in the data: prev becomes NaN because there’s no earlier row.\n",
    "\n",
    "    # percent change (safe)\n",
    "    df[\"pct_change\"] = np.where(df[\"prev\"].fillna(0) == 0, np.nan, df[\"abs_change\"] / df[\"prev\"])\n",
    "\n",
    "    # robust z-score of abs_change within each group (using MAD)\n",
    "    #It takes a series of changes within one group (PIN) and scores how unusual each change is compared to that PIN’s own history.\n",
    "    #What it is computing -- A) med: median of the series  B) mad: median absolute deviation from the median\n",
    "    #robust z = (x − median) / (1.4826 * MAD)\n",
    "    def _robust_z(s: pd.Series) -> pd.Series:\n",
    "        med = np.nanmedian(s)\n",
    "        mad = np.nanmedian(np.abs(s - med))\n",
    "        if mad == 0 or np.isnan(mad):\n",
    "            return pd.Series(np.zeros(len(s)), index=s.index, dtype=float)\n",
    "        return (s - med) / (1.4826 * mad) #Evaluating the z_score while 1.4826 * MAD = Standard Deviation for normal distribution\n",
    "\n",
    "    df[\"z_change\"] = g[\"abs_change\"].transform(_robust_z).abs() #Passing absolute change series through _robust_z\n",
    "    #z_change ~ 0 --> Normal; z_change >=3 --> Significant anomaly\n",
    "\n",
    "\n",
    "    # cap pct_change to avoid infinite blow-ups from tiny denominators\n",
    "    pct = df[\"pct_change\"].abs().clip(lower=0, upper=20)  # bounded percent-change magnitude\n",
    "    \n",
    "    \n",
    "    #Severity score ranks regions by how unusually large and operationally impactful their recent activity changes are. \n",
    "   \n",
    "    df[\"severity\"] = (pct.fillna(0) + 0.1) * (df[\"z_change\"].fillna(0) + 0.1) * np.log1p(df[value_col])\n",
    "    #pct --> How big was the month-to-month change in percentage?\n",
    "    #z_change --> How unusual was this change for this PIN historically or how surprising is this change for this area?\n",
    "    #log(1+x) --> How big is the real-world impact of this anomaly without letting large regions overpower the analysis?\n",
    "    # If either pct or z_change is 0, the product would become 0 and you’d lose information.+ 0.1 makes severity still non-zero for mild anomalies and prevents hard zeros.\n",
    "\n",
    "    # flag spikes (TRUE if)\n",
    "    #Condition 1: Top 1% by severity globally --> severity >= 99th percentile, OR\n",
    "    #Condition 2: z_change >= 3 (very unusual for that PIN) AND pct >= 1 (≥ 100% change)\n",
    "    \n",
    "    df[\"is_spike\"] = (df[\"severity\"] >= df[\"severity\"].quantile(0.99)) | ((df[\"z_change\"] >= 3) & (pct >= 1))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89a456a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply on PIN-level (best for anomaly surfacing)\n",
    "bio_spikes_pin  = add_spike_metrics(bio_pin_m,  group_keys=[\"state\",\"district\",\"pincode\"], value_col=\"total\") #Aadhar Biometric spikes at PIN level\n",
    "demo_spikes_pin = add_spike_metrics(demo_pin_m, group_keys=[\"state\",\"district\",\"pincode\"], value_col=\"total\") #Aadhar Demographic spikes at PIN level\n",
    "enr_spikes_pin  = add_spike_metrics(enr_pin_m,  group_keys=[\"state\",\"district\",\"pincode\"], value_col=\"total\") #Aadhar Enrollment spikes at PIN level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135d0277",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Apply on District-level (best for anomaly surfacing)\n",
    "bio_spikes_district  = add_spike_metrics(bio_dist_m,  group_keys=[\"state\",\"district\"], value_col=\"total\") #Aadhar Biometric spikes at district level\n",
    "demo_spikes_district = add_spike_metrics(demo_dist_m, group_keys=[\"state\",\"district\"], value_col=\"total\") #Aadhar Demographic spikes at district level\n",
    "enr_spikes_district  = add_spike_metrics(enr_dist_m,  group_keys=[\"state\",\"district\"], value_col=\"total\") #Aadhar Enrollment spikes at district level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6fa46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Severity in descending order (Aadhar enrollment spikes at PIN level)\n",
    "enr_spikes_pin[enr_spikes_pin[\"is_spike\"] == True].sort_values(\"severity\", ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4b26ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Severity in descending order (Aadhar enrollment spikes at District level)\n",
    "enr_spikes_district[enr_spikes_district[\"is_spike\"] == True].sort_values(\"severity\", ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e25d13f",
   "metadata": {},
   "source": [
    "### **Top anomalies by month**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be02e94e",
   "metadata": {},
   "source": [
    "Anomalies typically represent unusual surges, drops, or disruptions that cannot be explained by normal variability. In this analysis, anomaly detection is applied at a granular month-level to measure how different a given month’s activity is compared to that location’s own historical behavior, rather than against a global average.\n",
    "\n",
    "Methodologically, the approach combines month-to-month change, relative percentage change, and a robust z-score based on the median absolute deviation (MAD), which is well established in the literature as a stable alternative to standard deviation when data contain outliers or non-normal distributions. This allows the model to detect true operational shocks while remaining resistant to noise and scale effects across districts of different sizes. Each detected anomaly is further quantified using a severity score, which integrates the magnitude of change, its statistical rarity, and the underlying activity volume, ensuring that both unusualness and real-world impact are captured.\n",
    "\n",
    "The utility of anomaly detection lies in its ability to produce actionable, time-specific signals—highlighting when and where abnormal events occur, such as special enrolment drives, system outages, or backlog clearances. These anomaly signals form the evidentiary layer of the analysis and are subsequently aggregated to inform higher-level risk metrics, such as spike frequency and severity in the Instability Score. In this way, anomaly detection serves as the event-level diagnostic foundation, complementing district-level instability analysis and enabling both immediate operational response and longer-term planning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977224e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_anomalies_by_month(spike_df: pd.DataFrame, topn: int = 20) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns top N spike anomalies for each month ranked by severity (descending).\n",
    "    Works for both PIN-level spikes (has pincode) and district-level spikes (no pincode).\n",
    "    Only includes rows where is_spike == True.\n",
    "    \"\"\"\n",
    "    df = spike_df.copy()\n",
    "\n",
    "    # keep valid months and only spikes\n",
    "    df = df[df[\"month\"].notna()].copy()\n",
    "    df = df[df[\"is_spike\"] == True].copy()\n",
    "\n",
    "    if df.empty:\n",
    "        # return empty dataframe with whatever columns would have existed\n",
    "        return df\n",
    "\n",
    "    # rank within month by severity (highest = rank 1)\n",
    "    df[\"rank_in_month\"] = df.groupby(\"month\")[\"severity\"].rank(method=\"first\", ascending=False)\n",
    "\n",
    "    out = (\n",
    "        df[df[\"rank_in_month\"] <= topn]\n",
    "        .sort_values([\"month\", \"rank_in_month\"], ascending=[True, True])\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    # Choose identifier columns based on availability\n",
    "    id_cols = [\"month\", \"state\", \"district\"]\n",
    "    if \"pincode\" in out.columns:\n",
    "        id_cols.append(\"pincode\")\n",
    "\n",
    "    # Common metric columns (keep only those that exist)\n",
    "    metric_cols = [\n",
    "        \"total\", \"prev\", \"abs_change\", \"pct_change\", \"z_change\",\n",
    "        \"severity\", \"is_spike\", \"rank_in_month\"\n",
    "    ]\n",
    "    keep_cols = [c for c in (id_cols + metric_cols) if c in out.columns]\n",
    "\n",
    "    return out[keep_cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14d2959",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Anomaly detection at Pincode level\n",
    "bio_top_anoms_by_pin_month  = top_anomalies_by_month(bio_spikes_pin,  topn=20) #topn can change\n",
    "demo_top_anoms_by_pin_month = top_anomalies_by_month(demo_spikes_pin, topn=20)\n",
    "enr_top_anoms_by_pin_month  = top_anomalies_by_month(enr_spikes_pin,  topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a37741",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Anomaly detection at District level\n",
    "bio_top_anoms_by_dist_month  = top_anomalies_by_month(bio_spikes_district,  topn=20)\n",
    "demo_top_anoms_by_dist_month = top_anomalies_by_month(demo_spikes_district, topn=20)\n",
    "enr_top_anoms_by_dist_month  = top_anomalies_by_month(enr_spikes_district,  topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0139fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "enr_top_anoms_by_dist_month.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cf93a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "enr_top_anoms_by_pin_month.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8760577a",
   "metadata": {},
   "source": [
    "### **Instability Score**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fddd68",
   "metadata": {},
   "source": [
    "Instability Score is a composite indicator that summarizes how operationally unpredictable a district’s Aadhaar activity is over time. Rather than relying on raw monthly volumes, it integrates four complementary signals: \n",
    "\n",
    "* volatility (how much monthly activity fluctuates relative to its average), \n",
    "* spike frequency (how often abnormal surges occur), \n",
    "* peak stress (how extreme the highest-load month is compared to normal), and \n",
    "* spike intensity (how severe those abnormal surges are when they occur). \n",
    "\n",
    "Together, these dimensions capture both chronic instability and event-driven risk, an approach widely used in operations and risk management literature where composite indices are favored for their robustness and interpretability over single metrics.\n",
    "\n",
    "For a decision maker, the Instability Score functions as a prioritization and planning tool. A higher score indicates districts or pins that are harder to manage operationally and more prone to service disruptions without proactive intervention.\n",
    "\n",
    "The score translates complex time-series behavior into a single, actionable measure that supports informed resource allocation, risk mitigation, and policy decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9da13dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    level=\"district\" -> one row per (state, district)\n",
    "    level=\"pin\"      -> one row per (state, district, pincode)\n",
    "\n",
    "    Returns one row per (state, district/pin) with:\n",
    "      - cov (weight = 0.30): {Std/Mean}, \n",
    "      - spike_freq (weight = 0.25): {is_spike = TRUE / no of months}, \n",
    "      - peak_factor (weight = 0.20): {Max Total / Mean}\n",
    "      - severity metric (weight = 0.25): aggregated severity of spikes in that district\n",
    "      - normalized components\n",
    "      - final instability_score (0-100)\n",
    "\n",
    "    Inputs:\n",
    "      monthly_df: output of monthly_aggregate(..., level=\"district\") OR similar\n",
    "      spike_df: output of add_spike_metrics() at district or pin level\n",
    "\n",
    "     severity_agg:\n",
    "      - sum: total severity across spiky PINs in a district-month (captures distributed stress)\n",
    "      - max: worst spike in that district-month (captures extreme local pocket)\n",
    "      - mean: average severity across spiky PINs (captures typical spike intensity), how intense is a typical spike pocket?\n",
    "      \n",
    "    severity_stat = mean: \n",
    "        What it emphasizes\n",
    "        - Chronic, everyday stress\n",
    "        - Repeated moderate problems\n",
    "        - Long-term operational load\n",
    "        What it ignores\n",
    "        - Rare but very dangerous months\n",
    "        - Extreme one-off stress events\n",
    "    severity_stat = p95:\n",
    "        What it emphasizes\n",
    "        - Risk of extreme stress events\n",
    "        - Preparedness for worst-case scenarios\n",
    "        - Planning for rare but high-impact spikes\n",
    "        What it ignores\n",
    "        - Everyday operational load\n",
    "        - Chronic moderate problems\n",
    "        - Normal months\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def _robust_minmax(s: pd.Series, low_q=0.05, high_q=0.95) -> pd.Series:\n",
    "    s = s.astype(float)\n",
    "    lo = s.quantile(low_q)\n",
    "    hi = s.quantile(high_q)\n",
    "    if pd.isna(lo) or pd.isna(hi) or hi == lo:\n",
    "        return pd.Series(np.zeros(len(s)), index=s.index, dtype=float)\n",
    "    return (s.clip(lo, hi) - lo) / (hi - lo)\n",
    "\n",
    "def instability_score_with_severity(\n",
    "    monthly_df: pd.DataFrame,\n",
    "    spike_df: pd.DataFrame,\n",
    "    level: str = \"district\",            # \"district\" or \"pin\"\n",
    "    value_col: str = \"total\",\n",
    "    min_months: int = 4,\n",
    "    weights: dict | None = None,\n",
    "    severity_agg: str = \"sum\",          # \"sum\" or \"max\" or \"mean\"\n",
    "    severity_stat: str = \"p95\",         # \"mean\" or \"p95\"\n",
    "    peak_mode: str = \"ratio\"            # \"ratio\" or \"log_ratio\"\n",
    ") -> pd.DataFrame:\n",
    "   \n",
    "    # ---- Defaults\n",
    "    if weights is None:\n",
    "        # Must sum to 1.0 for clean 0–100 interpretation\n",
    "        weights = {\"cov\": 0.30, \"spike_freq\": 0.25, \"peak\": 0.20, \"severity\": 0.25}\n",
    "\n",
    "    # ---- Validate mode params\n",
    "    if level not in {\"district\", \"pin\"}:\n",
    "        raise ValueError(\"level must be 'district' or 'pin'\")\n",
    "    if severity_agg not in {\"sum\", \"max\", \"mean\"}:\n",
    "        raise ValueError(\"severity_agg must be one of: 'sum', 'max', 'mean'\")\n",
    "    if severity_stat not in {\"mean\", \"p95\"}:\n",
    "        raise ValueError(\"severity_stat must be one of: 'mean', 'p95'\")\n",
    "    if peak_mode not in {\"ratio\", \"log_ratio\"}:\n",
    "        raise ValueError(\"peak_mode must be 'ratio' or 'log_ratio'\")\n",
    "\n",
    "    # ---- Group keys\n",
    "    keys = [\"state\", \"district\"] if level == \"district\" else [\"state\", \"district\", \"pincode\"]\n",
    "\n",
    "    # ---- Validate required columns\n",
    "    required_monthly = set(keys + [\"month\", value_col])\n",
    "    missing_monthly = required_monthly - set(monthly_df.columns)\n",
    "    if missing_monthly:\n",
    "        raise ValueError(f\"monthly_df missing required columns: {missing_monthly}\")\n",
    "\n",
    "    required_spike = set(keys + [\"month\", \"is_spike\", \"severity\"])\n",
    "    missing_spike = required_spike - set(spike_df.columns)\n",
    "    if missing_spike:\n",
    "        raise ValueError(f\"spike_df missing required columns: {missing_spike}\")\n",
    "\n",
    "    # ---- Sort monthly data\n",
    "    df = monthly_df.copy().sort_values(keys + [\"month\"])\n",
    "\n",
    "    # 1) Base metrics per group\n",
    "    base = (\n",
    "        df.groupby(keys, as_index=False)\n",
    "          .agg(\n",
    "              months=(\"month\", \"nunique\"),\n",
    "              mean=(value_col, \"mean\"),\n",
    "              std=(value_col, \"std\"),\n",
    "              max_total=(value_col, \"max\"),\n",
    "          )\n",
    "    )\n",
    "\n",
    "    base[\"cov\"] = np.where(base[\"mean\"] == 0, np.nan, base[\"std\"] / base[\"mean\"])\n",
    "    base[\"peak_factor\"] = np.where(base[\"mean\"] == 0, np.nan, base[\"max_total\"] / base[\"mean\"])\n",
    "    if peak_mode == \"log_ratio\":\n",
    "        base[\"peak_factor\"] = np.log1p(base[\"peak_factor\"])\n",
    "\n",
    "    # Ensure enough history\n",
    "    base = base[base[\"months\"] >= min_months].copy()\n",
    "\n",
    "    # 2) Spike frequency (align spikes to months present in monthly_df)\n",
    "    spikes_only = spike_df[spike_df[\"is_spike\"] == True].copy()\n",
    "\n",
    "    # Keep only months that exist in monthly_df for the same group (prevents time-range mismatch)\n",
    "    valid_months = df[keys + [\"month\"]].drop_duplicates()\n",
    "    spikes_only = spikes_only.merge(valid_months, on=keys + [\"month\"], how=\"inner\")\n",
    "\n",
    "    spike_months = (\n",
    "        spikes_only.drop_duplicates(subset=keys + [\"month\"])\n",
    "        .groupby(keys, as_index=False)\n",
    "        .agg(spike_months=(\"month\", \"nunique\"))\n",
    "    )\n",
    "\n",
    "    out = base.merge(spike_months, on=keys, how=\"left\")\n",
    "    out[\"spike_months\"] = out[\"spike_months\"].fillna(0).astype(int)\n",
    "    out[\"spike_freq\"] = np.where(out[\"months\"] == 0, 0.0, out[\"spike_months\"] / out[\"months\"])\n",
    "\n",
    "    # 3) Severity metric\n",
    "    # Aggregate severity at group-month level\n",
    "    if severity_agg == \"sum\":\n",
    "        group_month_sev = spikes_only.groupby(keys + [\"month\"], as_index=False)[\"severity\"].sum()\n",
    "    elif severity_agg == \"max\":\n",
    "        group_month_sev = spikes_only.groupby(keys + [\"month\"], as_index=False)[\"severity\"].max()\n",
    "    else:  # mean\n",
    "        group_month_sev = spikes_only.groupby(keys + [\"month\"], as_index=False)[\"severity\"].mean()\n",
    "\n",
    "    group_month_sev = group_month_sev.rename(columns={\"severity\": \"group_month_severity\"})\n",
    "\n",
    "    # Summarize across months per group\n",
    "    if severity_stat == \"mean\":\n",
    "        sev = (\n",
    "            group_month_sev.groupby(keys, as_index=False)[\"group_month_severity\"]\n",
    "            .mean()\n",
    "            .rename(columns={\"group_month_severity\": \"severity_metric\"})\n",
    "        )\n",
    "    else:  # p95\n",
    "        sev = (\n",
    "            group_month_sev.groupby(keys, as_index=False)[\"group_month_severity\"]\n",
    "            .quantile(0.95)\n",
    "            .rename(columns={\"group_month_severity\": \"severity_metric\"})\n",
    "        )\n",
    "\n",
    "    out = out.merge(sev, on=keys, how=\"left\")\n",
    "    out[\"severity_metric\"] = out[\"severity_metric\"].fillna(0.0)\n",
    "\n",
    "    # 4) Normalize components\n",
    "    out[\"cov_norm\"] = _robust_minmax(out[\"cov\"].fillna(0))\n",
    "    out[\"spike_norm\"] = _robust_minmax(out[\"spike_freq\"].fillna(0))\n",
    "    out[\"peak_norm\"] = _robust_minmax(out[\"peak_factor\"].fillna(0))\n",
    "    out[\"severity_norm\"] = _robust_minmax(out[\"severity_metric\"].fillna(0))\n",
    "\n",
    "    # 5) Final score (0–100)\n",
    "    out[\"instability_score\"] = 100.0 * (\n",
    "        weights[\"cov\"] * out[\"cov_norm\"] +\n",
    "        weights[\"spike_freq\"] * out[\"spike_norm\"] +\n",
    "        weights[\"peak\"] * out[\"peak_norm\"] +\n",
    "        weights[\"severity\"] * out[\"severity_norm\"]\n",
    "    )\n",
    "\n",
    "    out = out.sort_values(\"instability_score\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "    cols = (\n",
    "        keys + [\n",
    "            \"months\", \"mean\", \"std\", \"cov\",\n",
    "            \"max_total\", \"peak_factor\",\n",
    "            \"spike_months\", \"spike_freq\",\n",
    "            \"severity_metric\",\n",
    "            \"cov_norm\", \"spike_norm\", \"peak_norm\", \"severity_norm\",\n",
    "            \"instability_score\"\n",
    "        ]\n",
    "    )\n",
    "    return out[cols]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b50dbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instability score at district level\n",
    "bio_dist_inst = instability_score_with_severity(\n",
    "    monthly_df=bio_dist_m,\n",
    "    spike_df=bio_spikes_district,\n",
    "    level=\"district\",\n",
    "    severity_agg=\"sum\",\n",
    "    severity_stat=\"p95\",\n",
    "    peak_mode=\"ratio\"\n",
    ")\n",
    "\n",
    "demo_dist_inst = instability_score_with_severity(\n",
    "    monthly_df=demo_dist_m,\n",
    "    spike_df=demo_spikes_district,\n",
    "    level=\"district\",\n",
    "    severity_agg=\"sum\",\n",
    "    severity_stat=\"p95\",\n",
    "    peak_mode=\"ratio\"\n",
    ")\n",
    "\n",
    "enr_dist_inst = instability_score_with_severity(\n",
    "    monthly_df=enr_dist_m,\n",
    "    spike_df=enr_spikes_district,\n",
    "    level=\"district\",\n",
    "    severity_agg=\"sum\",\n",
    "    severity_stat=\"p95\",\n",
    "    peak_mode=\"ratio\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779fe785",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instability for Enrollment dataset at district level\n",
    "enr_dist_inst.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb5f51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "enr_dist_inst[\"instability_score\"].value_counts(bins=10).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc552fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instability Scores for PINs\n",
    "bio_pin_inst = instability_score_with_severity(\n",
    "    monthly_df=bio_pin_m,\n",
    "    spike_df=bio_spikes_pin,\n",
    "    level=\"pin\",\n",
    "    severity_agg=\"sum\",\n",
    "    severity_stat=\"p95\",\n",
    "    peak_mode=\"ratio\"\n",
    ")\n",
    "\n",
    "demo_pin_inst = instability_score_with_severity(\n",
    "    monthly_df=demo_pin_m,\n",
    "    spike_df=demo_spikes_pin,\n",
    "    level=\"pin\",\n",
    "    severity_agg=\"sum\",\n",
    "    severity_stat=\"p95\",\n",
    "    peak_mode=\"ratio\"\n",
    ")\n",
    "\n",
    "enr_pin_inst = instability_score_with_severity(\n",
    "    monthly_df=enr_pin_m,\n",
    "    spike_df=enr_spikes_pin,\n",
    "    level=\"pin\",\n",
    "    severity_agg=\"sum\",\n",
    "    severity_stat=\"p95\",\n",
    "    peak_mode=\"ratio\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a832a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "enr_pin_inst[\"instability_score\"].value_counts(bins=10).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b249c8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instability for Enrollment dataset at Pincode level\n",
    "enr_pin_inst.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6f78c2",
   "metadata": {},
   "source": [
    "### **Top X contribution in instability**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82372cc",
   "metadata": {},
   "source": [
    "Top-X contribution analysis is a concentration metric used to quantify how much of a system’s total instability is driven by a small subset of units (e.g., districts or PIN codes). Rather than only identifying the most unstable entities, it measures the cumulative share of instability accounted for by the top X% of units, providing insight into the distributional structure of risk across the system.\n",
    "\n",
    "The primary objective of Top-X contribution analysis is to answer the question:\n",
    "\n",
    "*“Is instability widespread across the system, or is it concentrated in a small number of hotspots?”*\n",
    "\n",
    "By expressing instability in cumulative terms (e.g., “Top 10% of districts contribute 55% of total instability”), this metric enables:\n",
    "\n",
    "* Prioritization of interventions, by identifying whether targeting a small subset of districts can significantly reduce system-wide risk.\n",
    "* Efficient resource allocation, avoiding uniform responses when problems are structurally concentrated.\n",
    "* Strategic planning, by distinguishing between systemic instability and localized operational stress.\n",
    "\n",
    "This makes Top-X contribution especially valuable for decision makers who must choose where limited monitoring, staffing, or corrective capacity should be deployed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1711eee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def top_x_contribution(df: pd.DataFrame, value_col: str = \"instability_score\", x: float = 0.10) -> dict:\n",
    "    if not (0 < x <= 1):\n",
    "        raise ValueError(\"x must be between 0 and 1\")\n",
    "\n",
    "    s = pd.to_numeric(df[value_col], errors=\"coerce\").fillna(0.0).clip(lower=0)\n",
    "\n",
    "    if s.sum() == 0:\n",
    "        return {\"top_share_units_pct\": 0.0, \"contribution_share_pct\": 0.0, \"n_top_units\": 0}\n",
    "\n",
    "    sorted_vals = s.sort_values(ascending=False)\n",
    "    n = len(sorted_vals)\n",
    "    n_top = max(1, int(np.ceil(x * n)))\n",
    "\n",
    "    top_sum = sorted_vals.iloc[:n_top].sum()\n",
    "    total_sum = sorted_vals.sum()\n",
    "\n",
    "    return {\n",
    "        \"top_share_units_pct\": round(100 * n_top / n, 2),\n",
    "        \"contribution_share_pct\": round(100 * top_sum / total_sum, 2),\n",
    "        \"n_top_units\": n_top\n",
    "    }\n",
    "\n",
    "def gini_coefficient(values: pd.Series) -> float:\n",
    "    x = pd.to_numeric(values, errors=\"coerce\").fillna(0.0).to_numpy()\n",
    "    x = np.clip(x, 0, None)\n",
    "    if x.sum() == 0:\n",
    "        return 0.0\n",
    "    x = np.sort(x)\n",
    "    n = len(x)\n",
    "    cum = np.cumsum(x)\n",
    "    g = (n + 1 - 2 * (cum.sum() / cum[-1])) / n\n",
    "    return float(g)\n",
    "\n",
    "def _id_cols_for_level(df: pd.DataFrame, level: str) -> list[str]:\n",
    "    if level == \"district\":\n",
    "        return [\"district\", \"state\"]\n",
    "    if level == \"pin\":\n",
    "        # if state/district available, include them for context\n",
    "        cols = []\n",
    "        if \"pincode\" in df.columns:\n",
    "            cols.append(\"pincode\")\n",
    "        if \"district\" in df.columns:\n",
    "            cols.append(\"district\")\n",
    "        if \"state\" in df.columns:\n",
    "            cols.append(\"state\")\n",
    "        return cols if cols else [\"pincode\"]\n",
    "    raise ValueError(\"level must be 'district' or 'pin'\")\n",
    "\n",
    "def format_top_units(df: pd.DataFrame, level: str, topn: int = 5, value_col: str = \"instability_score\") -> str:\n",
    "    top = df.sort_values(value_col, ascending=False).head(topn)\n",
    "    id_cols = _id_cols_for_level(df, level)\n",
    "\n",
    "    parts = []\n",
    "    for _, r in top.iterrows():\n",
    "        label = \", \".join([f\"{c}={r[c]}\" for c in id_cols if c in top.columns])\n",
    "        parts.append(f\"{label} → {r[value_col]:.1f}\")\n",
    "    return \"; \".join(parts)\n",
    "\n",
    "def reason_mix(df: pd.DataFrame, col: str = \"reason_code\") -> str | None:\n",
    "    if col not in df.columns:\n",
    "        return None\n",
    "    vc = df[col].value_counts(normalize=True).head(5) * 100\n",
    "    return \", \".join([f\"{k}:{v:.0f}%\" for k, v in vc.items()])\n",
    "\n",
    "def generate_dataset_insight(\n",
    "    df_inst: pd.DataFrame,\n",
    "    dataset_name: str,\n",
    "    level: str = \"district\",            # \"district\" or \"pin\"\n",
    "    x_list: list[float] = [0.05, 0.10, 0.20],\n",
    "    topn: int = 5,\n",
    "    value_col: str = \"instability_score\"\n",
    ") -> str:\n",
    "    if value_col not in df_inst.columns:\n",
    "        raise ValueError(f\"{dataset_name}: '{value_col}' column not found\")\n",
    "\n",
    "    # Check id columns exist\n",
    "    if level == \"district\":\n",
    "        required = {\"state\", \"district\"}\n",
    "        missing = required - set(df_inst.columns)\n",
    "        if missing:\n",
    "            raise ValueError(f\"{dataset_name}: missing required district columns: {missing}\")\n",
    "    elif level == \"pin\":\n",
    "        if \"pincode\" not in df_inst.columns:\n",
    "            raise ValueError(f\"{dataset_name}: missing required pin column: 'pincode'\")\n",
    "    else:\n",
    "        raise ValueError(\"level must be 'district' or 'pin'\")\n",
    "\n",
    "    g = gini_coefficient(df_inst[value_col])\n",
    "\n",
    "    if g >= 0.50:\n",
    "        conc_word = \"highly concentrated\"\n",
    "    elif g >= 0.35:\n",
    "        conc_word = \"moderately concentrated\"\n",
    "    else:\n",
    "        conc_word = \"fairly distributed\"\n",
    "\n",
    "    conc_lines = []\n",
    "    for x in x_list:\n",
    "        res = top_x_contribution(df_inst, value_col=value_col, x=x)\n",
    "        conc_lines.append(f\"top {int(x*100)}% contribute {res['contribution_share_pct']}%\")\n",
    "\n",
    "    unit_word = \"districts\" if level == \"district\" else \"PIN codes\"\n",
    "    conc_text = \"; \".join(conc_lines)\n",
    "\n",
    "    top_text = format_top_units(df_inst, level=level, topn=topn, value_col=value_col)\n",
    "\n",
    "    mix = reason_mix(df_inst, col=\"reason_code\")\n",
    "    mix_text = f\" Reason mix: {mix}.\" if mix else \"\"\n",
    "\n",
    "    return (\n",
    "        f\"{dataset_name} ({unit_word}): Instability is {conc_word} (Gini={g:.2f}). \"\n",
    "        f\"Concentration: {conc_text} of total instability. \"\n",
    "        f\"Highest-risk {unit_word}: {top_text}.{mix_text}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e59faa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_dataset_insight(enr_dist_inst, \"Biometric Updates\", level=\"district\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03011e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_dataset_insight(enr_pin_inst, \"Biometric Updates\", level=\"pin\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e6efbc",
   "metadata": {},
   "source": [
    "### **K Means**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8497bcf5",
   "metadata": {},
   "source": [
    "We use K-means clustering here to discover natural groups of districts/PINs that behave similarly, rather than evaluating each location in isolation.\n",
    "\n",
    "**What K-means adds beyond ranking or scoring?**\n",
    "\n",
    "* Instability score ranks severity, but it does not explain why a district is unstable.\n",
    "* K-means clusters explain structure, grouping districts with similar instability drivers even if their total scores differ.\n",
    "\n",
    "Two districts/pin-codes may have the same instability score:\n",
    "\n",
    "* one driven by frequent small spikes,\n",
    "* another driven by a single extreme peak.\n",
    "\n",
    "K-means separates them into different clusters, enabling tailored interventions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a594ff6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def kmeans_cluster_districts(\n",
    "    df: pd.DataFrame,\n",
    "    feature_cols: list[str],\n",
    "    k: int | None = None,\n",
    "    k_min: int = 3,\n",
    "    k_max: int = 8,\n",
    "    random_state: int = 42\n",
    ") -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Adds a 'cluster' column using KMeans on feature_cols.\n",
    "    If k=None, chooses best k by silhouette.\n",
    "    Returns: (df_with_cluster, cluster_profile)\n",
    "    \"\"\"\n",
    "    out = df.copy()\n",
    "    X = out[feature_cols].fillna(0.0).to_numpy()\n",
    "\n",
    "    # Standardize for KMeans fairness\n",
    "    Xs = StandardScaler().fit_transform(X)\n",
    "\n",
    "    if k is None:\n",
    "        best_k, best_score = None, -1\n",
    "        for kk in range(k_min, k_max + 1):\n",
    "            model = KMeans(n_clusters=kk, n_init=30, random_state=random_state)\n",
    "            labels = model.fit_predict(Xs)\n",
    "            if len(np.unique(labels)) < 2:\n",
    "                continue\n",
    "            score = silhouette_score(Xs, labels)\n",
    "            if score > best_score:\n",
    "                best_score, best_k = score, kk\n",
    "        k = best_k if best_k is not None else 4\n",
    "\n",
    "    kmeans = KMeans(n_clusters=k, n_init=50, random_state=random_state)\n",
    "    out[\"cluster\"] = kmeans.fit_predict(Xs)\n",
    "\n",
    "    # Cluster profile for interpretation\n",
    "    profile = (\n",
    "        out.groupby(\"cluster\", as_index=False)\n",
    "           .agg(\n",
    "               n=(\"cluster\", \"size\"),\n",
    "               **{f\"avg_{c}\": (c, \"mean\") for c in feature_cols},\n",
    "               avg_instability=(\"instability_score\", \"mean\") if \"instability_score\" in out.columns else (\"cluster\",\"size\")\n",
    "           )\n",
    "           .sort_values(\"avg_instability\", ascending=False)\n",
    "           .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    return out, profile\n",
    "\n",
    "def plot_clusters_pca(\n",
    "    df: pd.DataFrame,\n",
    "    feature_cols: list[str],\n",
    "    title: str = \"District Clusters (PCA projection)\",\n",
    "    hover_cols: list[str] | None = None\n",
    "):\n",
    "    if hover_cols is None:\n",
    "        hover_cols = [\"state\", \"district\", \"instability_score\", \"reason_code\"]\n",
    "\n",
    "    plot_df = df.dropna(subset=feature_cols + [\"cluster\"]).copy()\n",
    "\n",
    "    # keep only hover columns that exist\n",
    "    hover_cols = [c for c in hover_cols if c in plot_df.columns]\n",
    "\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.decomposition import PCA\n",
    "    import plotly.express as px\n",
    "\n",
    "    X = plot_df[feature_cols].fillna(0.0).to_numpy()\n",
    "    Xs = StandardScaler().fit_transform(X)\n",
    "\n",
    "    pca = PCA(n_components=2, random_state=42)\n",
    "    coords = pca.fit_transform(Xs)\n",
    "    plot_df[\"pca_1\"] = coords[:, 0]\n",
    "    plot_df[\"pca_2\"] = coords[:, 1]\n",
    "\n",
    "    fig = px.scatter(\n",
    "        plot_df,\n",
    "        x=\"pca_1\",\n",
    "        y=\"pca_2\",\n",
    "        color=\"cluster\",\n",
    "        hover_data=hover_cols,\n",
    "        title=title\n",
    "    )\n",
    "    fig.update_layout(height=650)\n",
    "    fig.show()\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f504e970",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cluster_profile(profile_df: pd.DataFrame, feature_cols: list[str], title=\"Cluster Driver Profile\"):\n",
    "    # profile has avg_feature columns (avg_cov_norm etc.)\n",
    "    cols = [\"cluster\", \"n\"] + [f\"avg_{c}\" for c in feature_cols]\n",
    "    d = profile_df[cols].copy()\n",
    "\n",
    "    long = d.melt(id_vars=[\"cluster\",\"n\"], var_name=\"metric\", value_name=\"value\")\n",
    "    long[\"metric\"] = long[\"metric\"].str.replace(\"avg_\", \"\", regex=False)\n",
    "\n",
    "    fig = px.bar(long, x=\"metric\", y=\"value\", color=\"cluster\", barmode=\"group\",\n",
    "                 title=title, hover_data=[\"n\"])\n",
    "    fig.update_layout(height=520)\n",
    "    fig.show()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8107d56",
   "metadata": {},
   "source": [
    "**Plotting Enrollment District Instability**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead26d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"cov_norm\", \"spike_norm\", \"peak_norm\", \"severity_norm\"]  # drop severity_norm if not using it\n",
    "\n",
    "enr_inst_district_k, enr_inst_district_cluster_profile = kmeans_cluster_districts(enr_dist_inst, feature_cols=features, k=None)\n",
    "#print(enr_inst_district_cluster_profile)\n",
    "\n",
    "plot_clusters_pca(\n",
    "    enr_inst_district_k,\n",
    "    feature_cols=features,\n",
    "    title=\"Enrollment Instability — District Clusters\",\n",
    "    hover_cols=[\"state\", \"district\", \"instability_score\", \"cluster\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4648334",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cluster_profile(enr_inst_district_cluster_profile, features, title=\"Enrollment District Instability — Cluster Profiles\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f0fd72",
   "metadata": {},
   "source": [
    "### **Decoding cluster profile -- Aadhar Enrollment - District wise - Instability**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6eccacf",
   "metadata": {},
   "source": [
    "🔵 **Cluster 0 (Blue) — Stable / Low-Risk**\n",
    "\n",
    "*Profile: cov_norm≈0.075, spike_norm=0, peak_norm≈0.063, severity_norm=0*\n",
    "\n",
    "This cluster represents districts with highly stable enrollment behavior. Low normalized volatility and peak stress indicate that demand remains close to its typical level, while the absence of spikes and severity suggests no abnormal surges over the observed months. In operational terms, these districts behave predictably and require only routine monitoring and standard staffing. Cluster 0 can serve as a benchmark baseline for expected system behavior.\n",
    "\n",
    "Operational implications:\n",
    "\n",
    "* Maintain standard staffing and processes\n",
    "* Use as a benchmark baseline for expected performance\n",
    "* No intervention required beyond routine monitoring\n",
    "\n",
    "Probable Reason Code: STABLE\n",
    "\n",
    "🟣 **Cluster 1 (Purple) — Extreme Mixed Instability / High-Risk Hotspots**\n",
    "\n",
    "*Profile: cov_norm≈0.913, spike_norm≈0.644, peak_norm≈0.918, severity_norm≈0.874*\n",
    "\n",
    "This is the highest-risk cluster, characterized by simultaneous elevation across all four instability dimensions. High CoV indicates chronic month-to-month turbulence; high spike frequency shows that abnormal months occur repeatedly; high peak stress implies extreme capacity-shock months; and high severity confirms that spikes are operationally intense, not merely statistical noise. In practice, these are the districts most likely to trigger service congestion or SLA risk and should be prioritized for comprehensive intervention—buffer capacity, proactive forecasting, special monitoring, and root-cause investigation.\n",
    "\n",
    "Operational implications:\n",
    "\n",
    "* Highest priority for intervention\n",
    "* Deploy buffer capacity and surge staffing\n",
    "* Enable proactive monitoring and early-warning systems\n",
    "* Conduct root-cause analysis (infrastructure, demand shocks, policy drives)\n",
    "\n",
    "Probable Reason Code: MIXED\n",
    "\n",
    "🌸 **Cluster 2 (Pink) — Moderate Mixed Variability / Managed Instability**\n",
    "\n",
    "*Profile: cov_norm≈0.498, spike_norm≈0.556, peak_norm≈0.522, severity_norm≈0.136*\n",
    "\n",
    "Cluster 2 reflects districts with moderate instability driven by regular fluctuations and recurring spikes, but with low spike intensity. The combination of mid-level CoV, spike frequency, and peak factor suggests these areas experience operational variability (likely due to routine administrative cycles or predictable drives), yet the low severity indicates that spike events are generally contained and manageable. Operationally, this cluster benefits from improved scheduling and early-warning triggers rather than major capacity expansion.\n",
    "\n",
    "Operational implications:\n",
    "\n",
    "* Improve scheduling discipline and demand smoothing\n",
    "* Use rolling forecasts and adaptive staffing\n",
    "* Monitor trends rather than respond to incidents\n",
    "\n",
    "Probable Reason Code: VOLATILE\n",
    "\n",
    "🟠 **Cluster 3 (Orange) — Peaky-Volatile Without Spikes / Structural Swings**\n",
    "\n",
    "*Profile: cov_norm≈0.639, spike_norm=0, peak_norm≈0.656, severity_norm=0*\n",
    "\n",
    "This cluster shows structural variation and peak pressure without being classified as “spiky.” Higher volatility and peak factor indicate enrollment demand swings and elevated maximum months relative to baseline, but the absence of spike frequency/severity implies these changes are either gradual, seasonal, or do not meet the anomaly threshold used for spike classification. Operationally, this cluster represents districts where capacity planning should focus on seasonality management and forecasting, rather than incident response.\n",
    "\n",
    "Operational implications:\n",
    "\n",
    "* Plan seasonal or policy-driven capacity surges\n",
    "* Temporary staffing during known peak windows\n",
    "* Focus on capacity planning, not anomaly response\n",
    "\n",
    "Probable Reason Code: PEAKY\n",
    "\n",
    "🟡 **Cluster 4 (Yellow) — Spiky-Low-Volume / Event-Driven Surges**\n",
    "\n",
    "*Profile: cov_norm≈0.072, spike_norm≈0.619, peak_norm≈0.064, severity_norm≈0.150*\n",
    "\n",
    "Cluster 4 is defined by frequent spikes occurring on an otherwise stable baseline. Low CoV and peak factor suggest typical volumes are steady, but spike_norm is high—meaning spikes occur repeatedly. Severity is low-to-moderate, implying these spikes are not extremely intense but are persistent. This pattern is consistent with event-driven behavior (scheduled drives, periodic backlogs, outreach campaigns) that causes recurring surges without fundamentally destabilizing the baseline. Operationally, this cluster is best managed through event calendars, pre-emptive staffing for known spike windows, and queue management, rather than permanent capacity increase.\n",
    "\n",
    "Operational implications:\n",
    "\n",
    "* Plan seasonal or policy-driven capacity surges\n",
    "* Temporary staffing during known peak windows\n",
    "* Focus on capacity planning, not anomaly response\n",
    "\n",
    "Probable Reason Code: SPIKY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d08884a",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"cov_norm\", \"spike_norm\", \"peak_norm\", \"severity_norm\"]  # drop severity_norm if not using it\n",
    "\n",
    "enr_inst_pin_k, enr_pin_cluster_profile = kmeans_cluster_districts(enr_pin_inst, feature_cols=features, k=None)\n",
    "#print(enr_pin_cluster_profile)\n",
    "\n",
    "plot_clusters_pca(\n",
    "    enr_inst_pin_k,\n",
    "    feature_cols=features,\n",
    "    title=\"Enrollment Instability — Pincode Clusters\",\n",
    "    hover_cols=[\"state\", \"district\", \"pincode\", \"instability_score\", \"cluster\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d605397",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cluster_profile(enr_pin_cluster_profile, features, title=\"Enrollment Instability — Cluster Profiles (Pincode)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e905f1fc",
   "metadata": {},
   "source": [
    "### **Decoding cluster profile -- Aadhar Enrollment - Pincode wise - Instability**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055958ea",
   "metadata": {},
   "source": [
    "🔵 **Cluster 0 (Blue) — Extreme Mixed Instability Hotspots**\n",
    "\n",
    "*Profile: cov_norm ≈ 0.96, spike_norm ≈ 0.81, peak_norm ≈ 0.96, severity_norm ≈ 0.91*\n",
    "\n",
    "This cluster represents high-risk pincode hotspots where all dimensions of instability are simultaneously elevated. Enrollment activity in these pincodes is highly volatile month-to-month, experiences frequent spikes, reaches extreme peak loads, and those spikes are operationally severe. This pattern indicates localized breakdowns or intense operational stress, often driven by concentrated population demand, repeated special drives, or infrastructure constraints. These pincodes should be the top priority for intervention, requiring granular monitoring, surge capacity, and possibly structural fixes at the enrolment-center level.\n",
    "\n",
    "Operational implication: Requires deep root-cause analysis and multi-pronged intervention.\n",
    "\n",
    "Probable Reason Code: MIXED\n",
    "\n",
    "🌸 **Cluster 1 (Pink) — Stable Low-Activity Pincodes**\n",
    "\n",
    "*Profile: cov_norm ≈ 0.11, spike_norm = 0, peak_norm ≈ 0.09, severity_norm = 0*\n",
    "\n",
    "Cluster 1 captures pincodes with stable and predictable enrollment behavior. Low volatility and peak stress indicate consistent demand, and the complete absence of spikes and severity suggests no abnormal surges across the observation window. These pincodes form the operational baseline of the system. They require minimal intervention beyond routine operations and can be used as reference benchmarks when evaluating abnormal behavior elsewhere.\n",
    "\n",
    "Operational implication: Routine operations; no special action required.\n",
    "\n",
    "Probable Reason Code: STABLE\n",
    "\n",
    "\n",
    "🟠 **Cluster 2 (Orange) — Spiky Event-Driven Pincodes**\n",
    "\n",
    "*Profile: cov_norm ≈ 0.19, spike_norm ≈ 0.86, peak_norm ≈ 0.17, severity_norm ≈ 0.41*\n",
    "\n",
    "This cluster is characterized by frequent spike occurrences on an otherwise stable baseline. Volatility and peak stress remain low, but spike frequency is high, and severity is moderate—indicating repeated, localized surges that are noticeable but not catastrophic. These patterns are consistent with event-driven behavior, such as enrollment camps, periodic backlog clearances, or targeted outreach programs. Operationally, these pincodes benefit most from anticipatory planning (event calendars, temporary staffing) rather than permanent capacity expansion.\n",
    "\n",
    "Operational implication: Routine operations; no special action required.\n",
    "\n",
    "Probable Reason Code: SPIKY\n",
    "\n",
    "🟡 **Cluster 3 (Yellow) — Peaky but Non-Spiky Structural Load**\n",
    "\n",
    "*Profile: cov_norm ≈ 0.85, spike_norm = 0, peak_norm ≈ 0.84, severity_norm = 0*\n",
    "\n",
    "Cluster 3 represents pincodes with large structural swings and high peak months, but without being flagged as anomalous spikes. High volatility and peak factor suggest strong seasonal or cyclical demand patterns, yet the absence of spike frequency and severity implies these changes are gradual or expected, rather than sudden disruptions. These pincodes require forecast-driven capacity planning and seasonal staffing adjustments, not anomaly response mechanisms.\n",
    "\n",
    "Operational implication: Event-driven planning, early warnings, and short-term staffing buffers.\n",
    "\n",
    "Probable Reason Code: PEAKY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baafae22",
   "metadata": {},
   "source": [
    "### **Reason Code**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12a61c5",
   "metadata": {},
   "source": [
    "* **VOLATILE**\n",
    " \n",
    "    -Chronic month-to-month instability\n",
    "\n",
    "    -CoV high relative to other districts\n",
    "\n",
    "    -Interpretation: The district’s workload is structurally unstable (planning + staffing mismatch, inconsistent throughput, or irregular operations).\n",
    "\n",
    "* **SPIKY**\n",
    "\n",
    "    -Repeated abnormal events (drives, outages, backlog clearances).\n",
    "\n",
    "    -spike months occur often across the district timeline\n",
    "\n",
    "    -Interpretation: The district is usually stable, but repeatedly gets disrupted by episodic events.\n",
    "\n",
    "* **PEAKY**\n",
    "\n",
    "    -One or two extreme months causing capacity shock.\n",
    "\n",
    "    -A single “mountain” month. Flat before and after. That one month explains most of the perceived instability.\n",
    "\n",
    "    -Capacity planning problem, not chronic instability. You should plan temporary infrastructure / staff surge for known seasonal or campaign-like peak windows.\n",
    "\n",
    "* **MIXED**\n",
    "\n",
    "    -Multiple instability drivers → needs deeper investigation.\n",
    "\n",
    "    -Both structural instability and event shocks exist; needs deeper investigation and multi-pronged intervention.\n",
    "\n",
    "We decompose district instability into chronic variability (CoV), anomaly frequency (spike months), capacity shocks (peak factor), and spike intensity (severity). We assign a reason code only when one normalized driver clearly dominates; otherwise we label the district as MIXED to avoid false certainty and trigger deeper diagnostic review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f732c410",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_reason_code_with_severity(\n",
    "    df: pd.DataFrame,\n",
    "    margin: float = 0.10,\n",
    "    cov_col: str = \"cov_norm\",\n",
    "    spike_col: str = \"spike_norm\",\n",
    "    peak_col: str = \"peak_norm\",\n",
    "    severity_col: str = \"severity_norm\",\n",
    "    out_col: str = \"reason_code\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Assigns a dashboard-friendly reason code when Instability Score has 4 components:\n",
    "      - VOLATILE  : cov_norm dominates\n",
    "      - SPIKY     : spike_norm dominates (spike frequency)\n",
    "      - PEAKY     : peak_norm dominates (peak factor)\n",
    "      - INTENSE   : severity_norm dominates (spike intensity)\n",
    "      - MIXED     : no clear dominant driver (top-2 within `margin`)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    margin : float\n",
    "        If (top - second) <= margin, mark as MIXED.\n",
    "        Example: margin=0.10 means top driver must be >= 0.10 higher than second to \"dominate\".\n",
    "    \"\"\"\n",
    "    out = df.copy()\n",
    "\n",
    "    required = [cov_col, spike_col, peak_col, severity_col]\n",
    "    missing = [c for c in required if c not in out.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required normalized columns: {missing}\")\n",
    "\n",
    "    comps = out[required].to_numpy()\n",
    "\n",
    "    # Top and second-top values per row (district)\n",
    "    sorted_vals = np.sort(comps, axis=1)   # ascending\n",
    "    top = sorted_vals[:, -1]\n",
    "    second = sorted_vals[:, -2]\n",
    "\n",
    "    # Which component is maximum: 0=cov, 1=spike, 2=peak, 3=severity\n",
    "    argmax = np.argmax(comps, axis=1)\n",
    "\n",
    "    # MIXED if top two are close\n",
    "    is_mixed = (top - second) <= margin\n",
    "\n",
    "    labels = np.array([\"VOLATILE\", \"SPIKY\", \"PEAKY\", \"INTENSE\"], dtype=object)\n",
    "\n",
    "    out[out_col] = np.where(is_mixed, \"MIXED\", labels[argmax])\n",
    "\n",
    "    # Optional: dominant component name for debugging/explainability\n",
    "    dom_names = np.array([\"cov\", \"spike_freq\", \"peak_factor\", \"severity\"], dtype=object)\n",
    "    out[\"dominant_component\"] = np.where(is_mixed, \"mixed\", dom_names[argmax])\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90226e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reason codes for instability at district level\n",
    "bio_instability_district_reason1  = add_reason_code_with_severity(bio_dist_inst)\n",
    "demo_instability_district_reason1 = add_reason_code_with_severity(demo_dist_inst)\n",
    "enr_instability_district_reason1  = add_reason_code_with_severity(enr_dist_inst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c35dde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reason codes for instability at Pincode level\n",
    "bio_instability_pin_reason1  = add_reason_code_with_severity(bio_pin_inst)\n",
    "demo_instability_pin_reason1 = add_reason_code_with_severity(demo_pin_inst)\n",
    "enr_instability_pin_reason1  = add_reason_code_with_severity(enr_pin_inst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d00fa4",
   "metadata": {},
   "source": [
    "**Reason Code -- District wise**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75417f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "enr_instability_district_reason1.head(10)[[\"state\", \"district\", \"instability_score\", \"reason_code\",\"dominant_component\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270ec0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "enr_instability_district_reason1[\"reason_code\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b562d6e",
   "metadata": {},
   "source": [
    "**Reason Code -- Pincode wise**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddb7022",
   "metadata": {},
   "outputs": [],
   "source": [
    "enr_instability_pin_reason1.head(10)[[\"state\", \"district\", \"pincode\", \"instability_score\", \"reason_code\",\"dominant_component\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219e8d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "enr_instability_pin_reason1[\"reason_code\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9cc73e",
   "metadata": {},
   "source": [
    "### **Forecast next month load**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c357e1",
   "metadata": {},
   "source": [
    "Exponential Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4144bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast_next_month_ewma(\n",
    "    monthly_df: pd.DataFrame,\n",
    "    group_keys: list[str],\n",
    "    value_col: str = \"total\",\n",
    "    alpha: float = 0.5\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    One-step-ahead forecast using Simple Exponential Smoothing (EWMA).\n",
    "\n",
    "    Forecast(t+1) = alpha * y(t) + (1 - alpha) * Forecast(t)\n",
    "    Prediction interval is heuristic, based on EWMA std.\n",
    "    \"\"\"\n",
    "\n",
    "    # Defensive copy & proper ordering\n",
    "    df = monthly_df.copy().sort_values(group_keys + [\"month\"])\n",
    "    g = df.groupby(group_keys, sort=False)\n",
    "\n",
    "    # EWMA level (forecast signal)\n",
    "    df[\"ewma\"] = g[value_col].transform(\n",
    "        lambda s: s.ewm(alpha=alpha, adjust=False).mean()\n",
    "    )\n",
    "\n",
    "    # EWMA-based volatility (uncertainty signal)\n",
    "    df[\"ewm_std\"] = g[value_col].transform(\n",
    "        lambda s: s.ewm(alpha=alpha, adjust=False).std(bias=False)\n",
    "    )\n",
    "\n",
    "    # Take last observed month per group\n",
    "    last = df.groupby(group_keys, as_index=False).tail(1).copy()\n",
    "\n",
    "    # Forecast month = next calendar month\n",
    "    last[\"forecast_month\"] = (\n",
    "        last[\"month\"] + pd.offsets.MonthBegin(1)\n",
    "    ).dt.to_period(\"M\").dt.to_timestamp()\n",
    "\n",
    "    # One-step-ahead forecast\n",
    "    last[\"forecast\"] = last[\"ewma\"]\n",
    "\n",
    "    # Heuristic 95% prediction interval\n",
    "    last[\"pi_low\"] = (last[\"forecast\"] - 1.96 * last[\"ewm_std\"].fillna(0)).clip(lower=0)\n",
    "    last[\"pi_high\"] = (last[\"forecast\"] + 1.96 * last[\"ewm_std\"].fillna(0)).clip(lower=0)\n",
    "\n",
    "    cols = (\n",
    "        group_keys\n",
    "        + [\"forecast_month\", \"forecast\", \"pi_low\", \"pi_high\", \"month\", value_col]\n",
    "    )\n",
    "\n",
    "    return last[cols].rename(\n",
    "        columns={\"month\": \"last_month\", value_col: \"last_value\"}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837e052d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bio_forecast_district_next_es  = forecast_next_month_ewma(bio_dist_m,  group_keys=[\"state\",\"district\"], value_col=\"total\")\n",
    "demo_forecast_district_next_es = forecast_next_month_ewma(demo_dist_m, group_keys=[\"state\",\"district\"], value_col=\"total\")\n",
    "enr_forecast_district_next_es  = forecast_next_month_ewma(enr_dist_m,  group_keys=[\"state\",\"district\"], value_col=\"total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985bda42",
   "metadata": {},
   "outputs": [],
   "source": [
    "bio_forecast_pin_next_es  = forecast_next_month_ewma(bio_pin_m,  group_keys=[\"state\",\"district\",\"pincode\"], value_col=\"total\")\n",
    "demo_forecast_pin_next_es = forecast_next_month_ewma(demo_pin_m, group_keys=[\"state\",\"district\",\"pincode\"], value_col=\"total\")\n",
    "enr_forecast_pin_next_es  = forecast_next_month_ewma(enr_pin_m,  group_keys=[\"state\",\"district\",\"pincode\"], value_col=\"total\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b1880d",
   "metadata": {},
   "source": [
    "### **Top “consistent performers”**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179e8cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def consistent_performers(monthly_df: pd.DataFrame, level: str = \"district\",\n",
    "                          value_col: str = \"total\", min_months: int = 4,\n",
    "                          volume_quantile: float = 0.6, topn: int = 50) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Finds stable performers:\n",
    "    - low CoV and low median MoM % change\n",
    "    - but also above a volume threshold to avoid tiny-activity regions\n",
    "    \"\"\"\n",
    "    if level == \"district\":\n",
    "        keys = [\"state\",\"district\"]\n",
    "    elif level == \"pin\":\n",
    "        keys = [\"state\",\"district\",\"pincode\"]\n",
    "    else:\n",
    "        raise ValueError(\"level must be 'district' or 'pin'\")\n",
    "\n",
    "    df = monthly_df.copy().sort_values(keys + [\"month\"])\n",
    "    df[\"prev\"] = df.groupby(keys)[value_col].shift(1)\n",
    "    df[\"mom_abs_pct\"] = np.where(df[\"prev\"].fillna(0) == 0, np.nan, (df[value_col]-df[\"prev\"]).abs()/df[\"prev\"])\n",
    "\n",
    "    agg = (\n",
    "        df.groupby(keys, as_index=False)\n",
    "          .agg(\n",
    "              months=(\"month\",\"nunique\"),\n",
    "              total_sum=(value_col,\"sum\"),\n",
    "              mean=(value_col,\"mean\"),\n",
    "              std=(value_col,\"std\"),\n",
    "              median_mom_abs_pct=(\"mom_abs_pct\",\"median\"),\n",
    "              max_total=(value_col,\"max\"),\n",
    "          )\n",
    "    )\n",
    "    agg[\"cov\"] = np.where(agg[\"mean\"] == 0, np.nan, agg[\"std\"]/agg[\"mean\"])\n",
    "\n",
    "    agg = agg[agg[\"months\"] >= min_months].copy()\n",
    "    vol_thresh = agg[\"total_sum\"].quantile(volume_quantile)\n",
    "    agg = agg[agg[\"total_sum\"] >= vol_thresh].copy()\n",
    "\n",
    "    # stability score: lower is better\n",
    "    agg[\"stability_score\"] = (agg[\"cov\"].fillna(0) * 0.7) + (agg[\"median_mom_abs_pct\"].fillna(0) * 0.3)\n",
    "    agg = agg.sort_values([\"stability_score\",\"total_sum\"], ascending=[True, False]).head(topn).reset_index(drop=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d1ef73",
   "metadata": {},
   "outputs": [],
   "source": [
    "bio_consistent_districts  = consistent_performers(bio_dist_m,  level=\"district\", value_col=\"total\", min_months=4, volume_quantile=0.6, topn=50)\n",
    "demo_consistent_districts = consistent_performers(demo_dist_m, level=\"district\", value_col=\"total\", min_months=4, volume_quantile=0.6, topn=50)\n",
    "enr_consistent_districts  = consistent_performers(enr_dist_m,  level=\"district\", value_col=\"total\", min_months=4, volume_quantile=0.6, topn=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f22075",
   "metadata": {},
   "outputs": [],
   "source": [
    "enr_consistent_districts.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb40af33",
   "metadata": {},
   "outputs": [],
   "source": [
    "bio_consistent_pin  = consistent_performers(bio_pin_m,  level=\"pin\", value_col=\"total\", min_months=4, volume_quantile=0.6, topn=50)\n",
    "demo_consistent_pin = consistent_performers(demo_pin_m, level=\"pin\", value_col=\"total\", min_months=4, volume_quantile=0.6, topn=50)\n",
    "enr_consistent_pin  = consistent_performers(enr_pin_m,  level=\"pin\", value_col=\"total\", min_months=4, volume_quantile=0.6, topn=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fcdba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "enr_consistent_pin.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c0811c",
   "metadata": {},
   "source": [
    "### **Save outputs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3d7e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_DIR = r\"C:\\Users\\Abhay\\OneDrive\\Desktop\\Hackathons\\UIDAI\\Output\"\n",
    "#Aadhar Enrollment spikes\n",
    "enr_spikes_pin.to_csv(f\"{OUT_DIR}/enr_spikes_pin.csv\", index=False)\n",
    "enr_spikes_district.to_csv(f\"{OUT_DIR}/enr_spikes_district.csv\", index=False)\n",
    "\n",
    "#Aadhar Enrollment Anomalies\n",
    "enr_top_anoms_by_dist_month.to_csv(f\"{OUT_DIR}/enr_top_anoms_by_dist_month.csv\", index=False)\n",
    "enr_top_anoms_by_pin_month.to_csv(f\"{OUT_DIR}/enr_top_anoms_by_pin_month.csv\", index=False)\n",
    "\n",
    "#Aadhar Enrollment instability\n",
    "enr_pin_inst.to_csv(f\"{OUT_DIR}/enr_instability_pin.csv\", index=False)\n",
    "enr_dist_inst.to_csv(f\"{OUT_DIR}/enr_instability_district.csv\", index=False)\n",
    "\n",
    "#Aadhar Enrollment reason codes\n",
    "enr_instability_district_reason1.to_csv(f\"{OUT_DIR}/enr_instability_district_reason.csv\", index=False)\n",
    "enr_instability_pin_reason1.to_csv(f\"{OUT_DIR}/enr_instability_pin_reason.csv\", index=False)\n",
    "\n",
    "#Aadhar Enrollment Forecast\n",
    "enr_forecast_district_next_es.to_csv(f\"{OUT_DIR}/enr_forecast_district_next_es.csv\", index=False)\n",
    "enr_forecast_pin_next_es.to_csv(f\"{OUT_DIR}/enr_forecast_pin_next_es.csv\", index=False)\n",
    "\n",
    "#Aadhar Enrollment consistent performers\n",
    "enr_consistent_districts.to_csv(f\"{OUT_DIR}/enr_consistent_districts.csv\", index = False)\n",
    "enr_consistent_pin.to_csv(f\"{OUT_DIR}/enr_consistent_pin.csv\", index = False)\n",
    "\n",
    "print(\"Done. Key outputs created:\")\n",
    "print(f\"CSVs saved under: {OUT_DIR}/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
